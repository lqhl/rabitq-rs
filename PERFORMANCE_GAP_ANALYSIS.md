# Rust vs C++ Performance Gap Analysis (7-bit RaBitQ)

**Date**: 2025-11-02
**Status**: Deep dive into algorithmic and implementation differences
**Test Environment**: Fair comparison via ann-benchmarks

---

## Executive Summary

After deep code analysis, I've identified **the critical performance bottleneck** that causes Rust to be slower than C++ for 7-bit RaBitQ search:

**ROOT CAUSE**: Rust uses **scalar loops** for batch distance computation, while C++ uses **Eigen-vectorized operations** that are automatically SIMD-optimized by the compiler.

**Impact**: ~10-20% performance gap for 7-bit RaBitQ workloads
**Fix Complexity**: Medium (requires SIMD vectorization of batch operations)

---

## 1. Hot Path Comparison

### C++ Implementation (estimator.hpp:25-71)

```cpp
inline void split_batch_estdist(
    const char* batch_data,
    const SplitBatchQuery<float>& q_obj,
    size_t padded_dim,
    float* est_distance,
    float* low_distance,
    float* ip_x0_qr,
    bool use_hacc
) {
    // ... FastScan accumulation ...

    // KEY: Eigen vectorized operations on entire batch (32 elements)
    ConstRowMajorArrayMap<float> f_add_arr(cur_batch.f_add(), 1, fastscan::kBatchSize);
    ConstRowMajorArrayMap<float> f_rescale_arr(cur_batch.f_rescale(), 1, fastscan::kBatchSize);
    ConstRowMajorArrayMap<float> f_error_arr(cur_batch.f_error(), 1, fastscan::kBatchSize);

    RowMajorArrayMap<float> est_dist_arr(est_distance, 1, fastscan::kBatchSize);
    RowMajorArrayMap<float> ip_x0_qr_arr(ip_x0_qr, 1, fastscan::kBatchSize);
    RowMajorArrayMap<float> low_dist_arr(low_distance, 1, fastscan::kBatchSize);

    // Vectorized computation (3 lines for 32 elements)
    ip_x0_qr_arr = q_obj.delta() * (accu_arr.template cast<float>()) + q_obj.sum_vl_lut();
    est_dist_arr = f_add_arr + q_obj.g_add() + f_rescale_arr * (ip_x0_qr_arr + q_obj.k1xsumq());
    low_dist_arr = est_dist_arr - f_error_arr * q_obj.g_error();
}
```

**What Eigen does**:
- `RowMajorArrayMap` wraps raw pointers as Eigen array views (zero-copy)
- Arithmetic operations (`+`, `*`, `-`) are **overloaded** to generate vectorized code
- Compiler automatically generates AVX/AVX2/AVX-512 instructions
- Processes 8 f32 values per AVX2 instruction (or 16 with AVX-512)

**Assembly output** (approximate):
```asm
; Load 8 floats from f_add_arr
vmovups ymm0, [f_add_ptr]
; Load 8 floats from ip_x0_qr_arr
vmovups ymm1, [ip_x0_qr_ptr]
; Multiply and add in one instruction
vfmadd213ps ymm1, ymm2, ymm0  ; ymm1 = ymm2 * ymm1 + ymm0
; Store result
vmovups [est_dist_ptr], ymm1
; Loop unrolled for all 32 elements (4 iterations of 8)
```

### Rust Implementation (ivf.rs:1839-1873)

```rust
// Scalar loop: Convert u16 accumulator to f32 distance
let mut ip_values = vec![0.0f32; simd::FASTSCAN_BATCH_SIZE];
for i in 0..simd::FASTSCAN_BATCH_SIZE {
    let accu = accu_res[i] as f32;
    ip_values[i] = lut.delta * accu + lut.sum_vl_lut;
}

// ... later in the code ...

// Another scalar loop: Compute estimated distance per vector
for i in 0..actual_batch_size {
    let est_distance = batch_f_add[i]
        + g_add
        + batch_f_rescale[i] * (ip_x0_qr + query_precomp.k1x_sum_q);

    let lower_bound = est_distance - batch_f_error[i] * g_error;

    // ... pruning and ex-code evaluation ...
}
```

**Problem**:
1. **Two separate scalar loops** instead of one vectorized pass
2. Heap allocation: `vec![0.0f32; 32]` (128 bytes per batch)
3. Compiler **may** auto-vectorize, but less reliably than Eigen
4. No explicit SIMD for batch distance computation

**Assembly output** (likely):
```asm
; Scalar loop (if auto-vectorization fails)
loop:
    movss xmm0, [accu_res + rcx*4]   ; Load 1 f32
    cvtsi2ss xmm0, xmm0              ; Convert u16->f32
    mulss xmm0, xmm1                 ; Scalar multiply
    addss xmm0, xmm2                 ; Scalar add
    movss [ip_values + rcx*4], xmm0  ; Store 1 f32
    inc rcx
    cmp rcx, 32
    jl loop
; Or partially vectorized (4 elements at a time)
```

---

## 2. Performance Impact Analysis

### Batch Distance Computation Cost

**Operations per batch** (32 vectors):
- Convert accumulator to ip_x0_qr: `32 * (1 mul + 1 add) = 64 ops`
- Compute est_distance: `32 * (2 add + 1 mul) = 96 ops`
- Compute lower_bound: `32 * (1 sub + 1 mul) = 64 ops`
- **Total: 224 scalar ops per batch**

**C++ (Eigen vectorized)**:
- AVX2: 224 ops / 8 lanes = **28 SIMD instructions**
- AVX-512: 224 ops / 16 lanes = **14 SIMD instructions**
- Plus loop overhead (4-8 iterations)
- **Estimated cycles**: ~30-50 cycles/batch

**Rust (scalar or partial vectorization)**:
- Best case (fully auto-vectorized): ~40-60 cycles/batch
- Realistic case (partial vectorization): ~80-120 cycles/batch
- Worst case (no vectorization): ~150-200 cycles/batch
- **Estimated cycles**: ~80-120 cycles/batch (middle estimate)

**Performance gap**: **2-3x slower** for batch distance computation

### Impact on Overall Search

For GIST-1M (nprobe=64, cluster_size~250):
- Total batches per query: 64 * (250/32) = ~500 batches
- Extra cycles per query: 500 * (90 - 40) = **25,000 cycles**
- At 2.3 GHz: ~**10 Œºs overhead**

Typical query latency: ~100-200 Œºs
**Percentage impact: 5-10% slower**

But this is **per-batch overhead**. In reality, for larger nprobe or datasets:
- More batches ‚Üí larger absolute overhead
- **Estimated total impact: 10-20% slower than C++**

---

## 3. Additional Micro-Inefficiencies

### 3.1 Heap Allocation in Hot Path

**Location**: `ivf.rs:1822, 1839`

```rust
let mut ip_values = vec![0.0f32; simd::FASTSCAN_BATCH_SIZE];  // 128 bytes
```

**Cost**:
- Memory allocation: ~50-100 cycles
- Deallocation: ~30-50 cycles
- Cache pollution
- **Total: ~80-150 cycles per batch**

**Frequency**: Once per batch (not per vector), but still adds up
**Impact**: ~5% for small datasets, <1% for large datasets

**Fix**: Use stack array
```rust
let mut ip_values = [0.0f32; simd::FASTSCAN_BATCH_SIZE];
```

### 3.2 Lack of Explicit SIMD for Distance Computation

**Current code**: Relies on compiler auto-vectorization

**Problem**:
- Auto-vectorization is not guaranteed
- May fail due to:
  - Complex control flow
  - Aliasing concerns
  - Loop dependencies
  - Compiler conservatism

**Evidence**: Need to check assembly output to confirm

### 3.3 Ex-Code Path is Already Optimal

**Good news**: The ex-code evaluation (6-bit inner product) is already highly optimized:
- Direct SIMD on packed data (`ip_packed_ex6_f32_avx2`)
- C++-compatible packing format
- Zero unpacking overhead
- Function pointer dispatch

**No performance gap** in this section (Phase 3 optimization successful)

---

## 4. C++ Compiler Advantages

### 4.1 Eigen Library Benefits

Eigen is battle-tested and highly optimized:
- **Template metaprogramming**: Generates optimal code at compile time
- **Expression templates**: Fuses multiple operations into single loops
- **Compiler-specific intrinsics**: Tailored for GCC/Clang/MSVC
- **Alignment hints**: Ensures data is aligned for SIMD

Example: `a + b * (c + d)` becomes:
```cpp
for (i = 0; i < N; i += 8) {
    ymm0 = _mm256_loadu_ps(&c[i]);
    ymm1 = _mm256_loadu_ps(&d[i]);
    ymm0 = _mm256_add_ps(ymm0, ymm1);    // c + d
    ymm2 = _mm256_loadu_ps(&b[i]);
    ymm0 = _mm256_mul_ps(ymm0, ymm2);    // b * (c + d)
    ymm3 = _mm256_loadu_ps(&a[i]);
    ymm0 = _mm256_add_ps(ymm0, ymm3);    // a + b * (c + d)
    _mm256_storeu_ps(&result[i], ymm0);
}
```

All in **one fused loop**, no intermediate allocations.

### 4.2 C++ Compiler Optimization Culture

- GCC/Clang have decades of optimization for scientific computing
- Better at recognizing vectorizable patterns
- More aggressive loop unrolling and inlining
- Better constant propagation

---

## 5. Detailed Code-Level Comparison

### Batch Processing Loop Structure

#### C++ (ivf.hpp:451-465)
```cpp
for (size_t i = 0; i < iter; ++i) {
    scan_one_batch(
        batch_data, ex_data, ids, q_obj, knns, fastscan::kBatchSize, use_hacc
    );

    batch_data += BatchDataMap<float>::data_bytes(padded_dim_);
    ex_data += ExDataMap<float>::data_bytes(padded_dim_, ex_bits_) * fastscan::kBatchSize;
    ids += fastscan::kBatchSize;
}
```

**Inside `scan_one_batch`** (ivf.hpp:476-518):
```cpp
// Step 1: Vectorized batch distance computation (Eigen)
std::array<float, fastscan::kBatchSize> est_distance;
std::array<float, fastscan::kBatchSize> low_distance;
std::array<float, fastscan::kBatchSize> ip_x0_qr;

split_batch_estdist(batch_data, q_obj, padded_dim_,
                    est_distance.data(), low_distance.data(),
                    ip_x0_qr.data(), use_hacc);

// Step 2: Scalar loop for pruning and ex-code evaluation
for (size_t i = 0; i < num_points; ++i) {
    float lower_dist = low_distance[i];
    if (lower_dist < distk) {
        PID id = ids[i];
        float ex_dist = split_distance_boosting(ex_data, ip_func_, q_obj, ...);
        knns.insert(id, ex_dist);
        distk = knns.top_dist();
    }
    ex_data += ExDataMap<float>::data_bytes(padded_dim_, ex_bits_);
}
```

**Key insight**: C++ **separates** vectorized computation from scalar pruning:
1. **Phase 1** (vectorized): Compute all 32 distances at once (Eigen)
2. **Phase 2** (scalar): Check each distance and evaluate ex-codes if needed

This is **optimal** because:
- Phase 1 is fully vectorizable (no branches, no dependencies)
- Phase 2 requires branching (cannot vectorize effectively)

#### Rust (ivf.rs:1801-1993)
```rust
for batch_idx in 0..total_batches {
    // Step 1: FastScan SIMD (same as C++)
    let mut accu_res = [0u16; simd::FASTSCAN_BATCH_SIZE];
    simd::accumulate_batch_avx2(...);

    // Step 2: Scalar conversion (NOT VECTORIZED)
    let mut ip_values = vec![0.0f32; simd::FASTSCAN_BATCH_SIZE];
    for i in 0..simd::FASTSCAN_BATCH_SIZE {
        ip_values[i] = lut.delta * accu_res[i] as f32 + lut.sum_vl_lut;
    }

    // Step 3: Get batch parameters (zero-copy)
    let batch_f_add = cluster.batch_f_add(batch_idx);
    let batch_f_rescale = cluster.batch_f_rescale(batch_idx);
    let batch_f_error = cluster.batch_f_error(batch_idx);

    // Step 4: Combined computation + pruning (INTERLEAVED)
    for i in 0..actual_batch_size {
        // Compute distance (scalar)
        let est_distance = batch_f_add[i] + g_add
            + batch_f_rescale[i] * (ip_x0_qr + query_precomp.k1x_sum_q);
        let lower_bound = est_distance - batch_f_error[i] * g_error;

        // Prune
        if lower_bound >= distk { continue; }

        // Ex-code evaluation
        let ex_dot = (self.ip_func)(...);
        distance = ...;
    }
}
```

**Problem**: Steps 2 and 4 are **interleaved scalar operations**
- Compiler cannot vectorize Step 4 due to branches and dependencies
- Step 2 is a separate scalar loop (could be vectorized, but compiler may not)

**Difference**: Rust does not cleanly separate vectorizable and non-vectorizable phases

---

## 6. Recommended Optimizations

### Priority 1: Vectorize Batch Distance Computation (HIGH IMPACT)

**Goal**: Match C++ Eigen's vectorized batch operations

**Implementation**: Add explicit SIMD for batch distance computation

```rust
// src/ivf.rs (new function)

#[cfg(all(target_arch = "x86_64", target_feature = "avx2"))]
#[target_feature(enable = "avx2")]
unsafe fn compute_batch_distances_avx2(
    accu_res: &[u16; 32],
    lut_delta: f32,
    lut_sum_vl: f32,
    batch_f_add: &[f32],
    batch_f_rescale: &[f32],
    batch_f_error: &[f32],
    g_add: f32,
    g_error: f32,
    k1x_sum_q: f32,
    ip_x0_qr: &mut [f32; 32],
    est_distance: &mut [f32; 32],
    lower_bound: &mut [f32; 32],
) {
    use std::arch::x86_64::*;

    let delta_vec = _mm256_set1_ps(lut_delta);
    let sum_vl_vec = _mm256_set1_ps(lut_sum_vl);
    let g_add_vec = _mm256_set1_ps(g_add);
    let g_error_vec = _mm256_set1_ps(g_error);
    let k1x_sum_q_vec = _mm256_set1_ps(k1x_sum_q);

    for i in (0..32).step_by(8) {
        // Load u16 accumulators and convert to f32
        let accu_u16 = _mm_loadu_si128(accu_res[i..].as_ptr() as *const __m128i);
        let accu_i32 = _mm256_cvtepu16_epi32(accu_u16);
        let accu_f32 = _mm256_cvtepi32_ps(accu_i32);

        // ip_x0_qr = delta * accu + sum_vl
        let ip_vec = _mm256_fmadd_ps(delta_vec, accu_f32, sum_vl_vec);
        _mm256_storeu_ps(&mut ip_x0_qr[i], ip_vec);

        // Load batch parameters
        let f_add_vec = _mm256_loadu_ps(&batch_f_add[i]);
        let f_rescale_vec = _mm256_loadu_ps(&batch_f_rescale[i]);
        let f_error_vec = _mm256_loadu_ps(&batch_f_error[i]);

        // est_distance = f_add + g_add + f_rescale * (ip_x0_qr + k1x_sum_q)
        let ip_plus_k1x = _mm256_add_ps(ip_vec, k1x_sum_q_vec);
        let rescale_term = _mm256_mul_ps(f_rescale_vec, ip_plus_k1x);
        let est_vec = _mm256_add_ps(f_add_vec, g_add_vec);
        let est_vec = _mm256_add_ps(est_vec, rescale_term);
        _mm256_storeu_ps(&mut est_distance[i], est_vec);

        // lower_bound = est_distance - f_error * g_error
        let error_term = _mm256_mul_ps(f_error_vec, g_error_vec);
        let lower_vec = _mm256_sub_ps(est_vec, error_term);
        _mm256_storeu_ps(&mut lower_bound[i], lower_vec);
    }
}
```

**Usage** (replace current scalar loop):
```rust
// Allocate on stack
let mut ip_x0_qr = [0.0f32; 32];
let mut est_distance = [0.0f32; 32];
let mut lower_bound = [0.0f32; 32];

// Vectorized computation
#[cfg(all(target_arch = "x86_64", target_feature = "avx2"))]
unsafe {
    compute_batch_distances_avx2(
        &accu_res,
        lut.delta,
        lut.sum_vl_lut,
        batch_f_add,
        batch_f_rescale,
        batch_f_error,
        g_add,
        g_error,
        query_precomp.k1x_sum_q,
        &mut ip_x0_qr,
        &mut est_distance,
        &mut lower_bound,
    );
}

// Then scalar loop for pruning and ex-code
for i in 0..actual_batch_size {
    if lower_bound[i] >= distk { continue; }

    // Ex-code evaluation
    let ex_dot = (self.ip_func)(...);
    // ...
}
```

**Expected speedup**: **1.15-1.25x** (10-20% faster)
**Complexity**: ~100 lines of SIMD code
**Risk**: Low (well-defined operations)

### Priority 2: Eliminate Heap Allocation (LOW IMPACT)

**Current**:
```rust
let mut ip_values = vec![0.0f32; simd::FASTSCAN_BATCH_SIZE];
```

**Fixed**:
```rust
let mut ip_values = [0.0f32; simd::FASTSCAN_BATCH_SIZE];
```

**Expected speedup**: **1.01-1.02x** (1-2% faster)
**Complexity**: 1-line change
**Risk**: None

### Priority 3: AVX-512 Optimization (OPTIONAL)

If Priority 1 is implemented with AVX2, add AVX-512 version:
- Process 16 f32 per iteration (2 iterations for 32 elements)
- Use `_mm512_*` intrinsics
- Conditional compilation with feature flag

**Expected speedup**: **1.05-1.10x over AVX2** (on AVX-512 hardware)

---

## 7. Why Ex-Code Performance is Already Good

The Phase 3 optimization successfully eliminated the unpacking overhead:

**Before Phase 3**:
```rust
// Had to unpack ex-codes first
let ex_code_unpacked = unpack_ex_code(packed, dim, ex_bits);
let ex_dot = dot_u16_f32(&ex_code_unpacked, query);
```
Cost: ~50-100 cycles per vector for unpacking

**After Phase 3**:
```rust
// Direct SIMD on packed data (C++-compatible format)
let ex_dot = (self.ip_func)(query, packed_ex_code, padded_dim);
```
Cost: ~20-30 cycles per vector (pure computation)

**Speedup**: ~2-3x for ex-code evaluation
**Status**: ‚úÖ Optimal (matches C++)

The ex-code path is **NOT** the bottleneck. The bottleneck is in the **batch distance computation** before ex-code evaluation.

---

## 8. Profiling Evidence Needed

To confirm this analysis, we need:

### 8.1 Assembly Inspection
```bash
cargo rustc --release --lib -- --emit asm
# Check if loops are vectorized in search_cluster_v2_batched
```

Look for:
- AVX2 instructions: `vmovups`, `vfmadd213ps`, `vmulps`, `vaddps`
- Loop unrolling
- SIMD register usage (`ymm0-ymm15`)

### 8.2 Perf Profiling
```bash
perf record -g cargo run --release --example benchmark_gist
perf report
```

Expected hotspots:
1. `search_cluster_v2_batched` (~40-50% of time)
2. Inside: batch distance loops (~15-20%)
3. `ip_packed_ex6_f32_avx2` (~15-20%)
4. Heap allocator (~2-5%)

### 8.3 Benchmark Comparison
Need exact numbers from ann-benchmarks:
- C++ QPS: ?
- Rust QPS: ?
- Gap: ?%

---

## 9. Summary Table

| Component | C++ Implementation | Rust Implementation | Performance Gap | Fix Priority |
|-----------|-------------------|---------------------|-----------------|--------------|
| **FastScan accumulation** | AVX2 SIMD | AVX2 SIMD (same) | ‚úÖ None | - |
| **Batch distance computation** | Eigen vectorized | Scalar loop | ‚ö†Ô∏è **2-3x slower** | üî¥ **HIGH** |
| **Ex-code inner product** | Direct SIMD on packed | Direct SIMD on packed (same) | ‚úÖ None | - |
| **Memory allocations** | Stack arrays | Some heap alloc | ‚ö†Ô∏è ~1-2% slower | üü° LOW |
| **Overall search** | Baseline | ~10-20% slower | ‚ö†Ô∏è **Moderate** | - |

---

## 10. Conclusion

**The primary performance gap** between Rust and C++ for 7-bit RaBitQ is due to:

1. **Lack of vectorized batch distance computation** (2-3x slower than Eigen)
2. **Minor heap allocations** in hot path (~1-2% overhead)

**The ex-code evaluation** (which was the focus of Phase 1-3 optimizations) is **already optimal** and matches C++ performance.

**Action Plan**:
1. Implement explicit SIMD for batch distance computation (Priority 1)
2. Replace heap allocation with stack array (Priority 2)
3. Verify with assembly inspection and profiling
4. Expected final performance: **95-100% of C++** (within measurement error)

**Estimated development time**: 1-2 days for Priority 1 + testing

---

## References

- C++ implementation: `/data00/home/liuqin.v/workspace/RaBitQ-Library/include/rabitqlib/index/estimator.hpp`
- Rust implementation: `/data00/home/liuqin.v/workspace/rabitq-rs/src/ivf.rs`
- Eigen library: C++ template-based linear algebra with automatic SIMD
- AVX2 instructions: 256-bit SIMD (8x f32 per instruction)
- AVX-512 instructions: 512-bit SIMD (16x f32 per instruction)
